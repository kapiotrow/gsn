{"cells":[{"cell_type":"markdown","id":"ce1fa27b","metadata":{"id":"ce1fa27b"},"source":["# Lab 5 - our own Large (or not that large) Language Model (LLM)\n","\n","In this lab, we will build our own Large Language Model (LLM) from scratch … or, to be precise, a highly simplified version that will allow us to understand the basics without requiring enormous computational resources or massive datasets.\n","\n","We will of course use the transformer architecture, which we introduced in the previous class. But first, let’s go through some important concepts.\n","\n","\n","What is a Language Model (LM)?\n","\n","A language model is a probabilistic model that operates on sequences of words or characters and predicts the next word or character in a sequence based on the previous ones. Language models are used in many natural language processing (NLP) applications, such as machine translation, speech recognition, text generation, and many others.\n","\n","In recent years, transformer-based language models have achieved remarkable results across a wide range of NLP tasks. These models are trained on massive text datasets and are capable of generating and understanding long and coherent texts.\n","\n","In this lab, however, we will focus on two main types of transformer-based language models:\n","1. **Autoregressive Model**: This type of model generates text sequentially, predicting the next word based on the previous ones. An example is GPT (Generative Pre-trained Transformer), developed primarily by OpenAI. Autoregressive models are often used for tasks such as text generation, machine translation, and text completion.\n","2. **Masked Language Model**: This type of model is trained to predict missing words in a sentence, where some words are “masked” (hidden). A well-known example is BERT (Bidirectional Encoder Representations from Transformers), developed by Google. Masked models are often used for tasks such as text classification, sentiment analysis, and question answering.\n","\n","\n","How do these two models differ?\n","\n","The simplest difference lies in their architecture. Autoregressive models typically consist only of decoders, while masked models consist only of encoders. Of course, there are also hybrid approaches, such as T5 (Text-to-Text Transfer Transformer) and BART (Bidirectional and Auto-Regressive Transformers), which combine features of both. However, for the purpose of this lab, we will focus on the pure implementations.\n","\n","In autoregressive models (e.g. GPT), the decoder is trained to predict the next word in a sequence based on all previous words. To achieve this, masking is applied to prevent the decoder from accessing future words during training (causal masking, which we discussed in the previous class). During text generation, the model produces words one by one, using its previously generated output as context.\n","\n","In masked language models (e.g. BERT), the encoder is trained to predict masked words in a sentence by using context from both the left and the right. During training, some words are randomly masked, and the model learns to predict them based on the remaining words. Masked models are generally used for tasks that require understanding the full context of a sentence or document.\n","\n","<img src=https://raw.githubusercontent.com/vision-agh/DNN-Course-media/refs/heads/main/lab4_5_transformer/figures/gpt_vs_bert.png width=600>\n","\n","Although, based on what we have learned so far, we could implement both types of models, in this lab we will focus on the autoregressive model. It is more intuitive, easier to understand for beginners, and more interesting in the context of text generation.\n","\n","For those interested, I encourage you to explore and experiment with masked models such as BERT to broaden your knowledge of different language model architectures. For example, you can follow this tutorial: [link](https://medium.com/@adnanmasood/a-tiny-bert-style-model-from-scratch-a-detailed-exploration-5bc47d59bff5).\n","\n","Finally, the code below is partly adapted from this tutorial: [link](https://www.youtube.com/watch?v=kCc8FmEb1nY) and original paper [Attention is All You Need](https://arxiv.org/abs/1706.03762).\n"]},{"cell_type":"markdown","id":"ef22e173","metadata":{"id":"ef22e173"},"source":["## Import and Setup\n","\n","As usual, we start by importing the necessary libraries and setting up the device."]},{"cell_type":"code","execution_count":null,"id":"LsehB1zQUdLE","metadata":{"id":"LsehB1zQUdLE"},"outputs":[],"source":["# !pip install fitz tools PyMuPDF"]},{"cell_type":"code","execution_count":null,"id":"f07235ad","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11743,"status":"ok","timestamp":1760339691976,"user":{"displayName":"JAN ROSA","userId":"09972594920799623787"},"user_tz":-120},"id":"f07235ad","outputId":"7324c7bb-5eb0-4613-b3f1-7323f4dedadf"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrosa\u001b[0m (\u001b[33mdeep-neural-network-course\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import os, glob\n","\n","from typing import Optional\n","\n","import wandb\n","\n","wandb.login()  # Log in to your W&B account\n","\n","torch.manual_seed(42)  # For reproducibility\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')\n","\n","def make_causal_mask(seq_len):\n","    return torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)"]},{"cell_type":"markdown","id":"bec99c0a","metadata":{"id":"bec99c0a"},"source":["Since we are going to implement an autoregressive model, we will need to initialize and use the following components:\n","\n","1. `PositionalEncoding` class – for adding positional information to the input embeddings.\n","2. `make_causal_mask` function - for creating a causal mask that prevents the model from attending to future tokens during training.\n","3. `Head`, `MultiHeadAttention` and `DecoderBlock` classes - for constructing the architecture.\n","\n","If we prepared part1 correctly, we can simply import these components directly from part1.ipynb."]},{"cell_type":"code","execution_count":null,"id":"400b9edd","metadata":{"id":"400b9edd"},"outputs":[],"source":["# --- Positional Encoding ---\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div)\n","        pe[:, 1::2] = torch.cos(pos * div)\n","        self.register_buffer(\"pe\", pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","# Visualization of Positional Encoding\n","def plot_positional_encoding(pe, max_len=200):\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(pe.pe[0, :max_len].cpu(), cmap='viridis', aspect='auto')\n","    plt.colorbar()\n","    plt.xlabel('Embedding Dimension')\n","    plt.ylabel('Position')\n","    plt.title('Positional Encoding')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"56b78006","metadata":{"id":"56b78006"},"outputs":[],"source":["# --- Single Attention Head ---\n","class Head(nn.Module):\n","    def __init__(self, emb_size, head_size, dropout=0.0, bias=False):\n","        super().__init__()\n","        self.key = nn.Linear(emb_size, head_size, bias=bias)\n","        self.query = nn.Linear(emb_size, head_size, bias=bias)\n","        self.value = nn.Linear(emb_size, head_size, bias=bias)\n","        self.scale = math.sqrt(head_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.attn_weights = None\n","\n","    def forward(self, q, k, v, mask=None):\n","        Q = self.query(q)\n","        K = self.key(k)\n","        V = self.value(v)\n","        return self.scaled_dot_product_attention(Q, K, V, mask)\n","\n","    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n","        if mask is not None:\n","            scores = scores.masked_fill(mask == 0, float('-inf'))\n","        attn = torch.softmax(scores, dim=-1)\n","        self.attn_weights = attn.detach().cpu()\n","        attn = self.dropout(attn)\n","        out = torch.matmul(attn, V)\n","        return out"]},{"cell_type":"code","execution_count":null,"id":"931a2cf2","metadata":{"id":"931a2cf2"},"outputs":[],"source":["# --- Multi-Head Attention ---\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, emb_size, num_heads, dropout=0.0, bias=False):\n","        super().__init__()\n","        assert emb_size % num_heads == 0\n","        head_size = emb_size // num_heads\n","        self.heads = nn.ModuleList([Head(emb_size, head_size, dropout, bias) for _ in range(num_heads)])\n","        self.linear = nn.Linear(emb_size, emb_size, bias=bias)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, q, k, v, mask=None):\n","        out = torch.cat([h(q, k, v, mask) for h in self.heads], dim=-1)\n","        out = self.linear(out)\n","        out = self.dropout(out)\n","        return out\n","\n","    def get_attention_maps(self):\n","        return [h.attn_weights for h in self.heads if h.attn_weights is not None]"]},{"cell_type":"code","execution_count":null,"id":"88fbea82","metadata":{"id":"88fbea82"},"outputs":[],"source":["# --- Decoder Block ---\n","class DecoderBlock(nn.Module):\n","    def __init__(self, emb_size, num_heads, dropout=0.0, expansion=4, use_cross_attn=False):\n","        super().__init__()\n","        self.use_cross_attn = use_cross_attn\n","        self.self_attn = MultiHeadAttention(emb_size, num_heads, dropout)\n","        self.cross_attn = MultiHeadAttention(emb_size, num_heads, dropout) if use_cross_attn else None\n","        self.norm1 = nn.LayerNorm(emb_size)\n","        self.norm2 = nn.LayerNorm(emb_size) if use_cross_attn else None\n","        self.norm3 = nn.LayerNorm(emb_size)\n","        self.ff = nn.Sequential(\n","            nn.Linear(emb_size, emb_size * expansion),\n","            nn.ReLU(),\n","            nn.Linear(emb_size * expansion, emb_size),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x, enc_out: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None):\n","        if not self.use_cross_attn and enc_out is not None:\n","            raise ValueError(\"Cross-attention is not enabled in this DecoderBlock.\")\n","        self_attn_out = self.self_attn(x, x, x, tgt_mask)\n","        x = self.norm1(x + self_attn_out)\n","        if self.cross_attn is not None and enc_out is not None:\n","            cross_attn_out = self.cross_attn(x, enc_out, enc_out)\n","            x = self.norm2(x + cross_attn_out)\n","        ff_out = self.ff(x)\n","        x = self.norm3(x + ff_out)\n","        return x"]},{"cell_type":"markdown","id":"6dcddbcb","metadata":{"id":"6dcddbcb"},"source":["## Now let's implement our own autoregressive Mini-GPT model\n","\n","We will now implement our own autoregressive Mini-GPT model using the components we have prepared earlier.\n","\n","Compared to our previous Transformer implementation, this model introduces one additional parameter — `block_size`, which defines the maximum sequence length that the model can handle. This parameter is crucial for autoregressive models because it determines how many previous tokens the model can consider when predicting the next one.\n","\n","If we allowed the model to use all previous tokens without any limit, we could quickly run into memory issues, especially for long sequences. The `block_size` acts as a sliding context window to keep computation and memory manageable.\n","\n","#### Text generation with generate()\n","\n","We will also implement a generate() method for text generation. This method will allow the model to produce text autoregressively — one token at a time — starting from an initial input sequence.\n","\n","In addition, we will use a `temperature` parameter to control the randomness of the predictions.\n","- A higher `temperature` results in more random and creative outputs.\n","- A lower `temperature` makes the model’s predictions more focused and deterministic.\n","\n","> How does it work?\n","> Once we obtain the model’s output logits, we can convert them into probabilities using the softmax function. The temperature is applied by dividing the logits by the temperature value before softmax.\n","\n","After obtaining the probability distribution, we sample from it to select the next token.\n","\n","For example, if the logits for the next token are [2.0, 1.0, 0.1] and the temperature is set to 0.5, the scaled logits become [4.0, 2.0, 0.2].\n","Applying softmax to these scaled values results in a probability distribution that is more concentrated on the highest value — making it more likely that the token corresponding to the largest logit will be selected.\n","\n","- `__init_`:\n","    - Initializes the parameter `block_size`.\n","    - Initializes token and positional embeddings.\n","    - Creates a list of decoder blocks (note that `use_cross_attn=False`, since we use only the decoder architecture).\n","    - Adds a final linear layer to project the decoder output to the vocabulary size.\n","\n","- `forward`:\n","    Similar to the previous Transformer implementation:\n","    1. The input sequence `src` is passed through the embedding layer, positional encoding, and then through the decoder blocks. Since this is a decoder-only model, we use a `causal mask` and set `enc_out=None`.\n","    2. The output is projected to the vocabulary dimension using the final linear layer.\n","    3. We compute the cross-entropy loss inside the model for convenience.\n","        - If `targets` is None, the loss is set to None.\n","        - Otherwise, the predicted logits and target tokens are reshaped to match the expected format for `F.cross_entropy`:\n","            - `logits` -> shape `(batch_size * seq_len, vocab_size)`\n","            - `targets` -> shape `(batch_size * seq_len)`\n","    4. The forward() method returns both the logits and the loss.\n","\n","- `generate`:\n","    This method generates text autoregressively.\n","    1. It takes an initial input sequence `src` and generates `max_new_tokens` additional tokens.\n","    2. For each iteration:\n","        - Slice the input to ensure it does not exceed `block_size` (src[:, -self.block_size:] works even when shorter).\n","        - Create a causal mask for the current sequence length.\n","        - Pass the current input and mask to the model to obtain logits.\n","        - Extract the logits for the last token in the sequence (`logits[:, -1, :]`) and divide them by the temperature to adjust randomness.\n","        - If `top_k` is specified, apply top-k filtering to keep only the top `k` most likely tokens, setting the rest to negative infinity:\n","\n","            ```python\n","            v, _ = torch.topk(logits, top_k)\n","            logits[logits < v[:, [-1]]] = -float('Inf')\n","            ```\n","        - Apply softmax to obtain a probability distribution over the vocabulary.\n","        - Sample the next token from this distribution using `torch.multinomial`.\n","        - Concatenate the sampled token to `src` for the next iteration.\n","    3. Finally, the method returns the extended sequence containing both the original and generated tokens.    "]},{"cell_type":"code","execution_count":null,"id":"8ec223f3","metadata":{"id":"8ec223f3"},"outputs":[],"source":["# --- MiniGPT model ---\n","class MiniGPT(nn.Module):\n","    def __init__(self, voc_size, emb_size=64, num_heads=2, num_layers=1, block_size=32, use_cross_attn=False):\n","        super().__init__()\n","        self.block_size = block_size\n","        self.token_emb = nn.Embedding(voc_size, emb_size)\n","        self.pos_emb   = PositionalEncoding(emb_size, max_len=block_size)\n","\n","        self.blocks =  nn.ModuleList([DecoderBlock(emb_size, num_heads, use_cross_attn=use_cross_attn) for _ in range(num_layers)])\n","        self.fc_out =  nn.Linear(emb_size, voc_size)\n","\n","    def forward(self, src, targets=None, mask=None):\n","        src = self.token_emb(src)\n","        src = self.pos_emb(src)\n","        for block in self.blocks:\n","            src = block(src)\n","        logits = self.fc_out(src)\n","        if targets is None:\n","            return logits, None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B * T, C)\n","            targets = targets.view(B * T)\n","            loss = F.cross_entropy(logits, targets)\n","            return logits, loss\n","\n","\n","    def generate(self, src, max_new_tokens, temperature=1.0, top_k=None):\n","        for _ in range(max_new_tokens):                             # 1. Loop for the number of new tokens to generate\n","            src_cut = src[:, -self.block_size:]                     # 2. Crop src to the last block_size tokens\n","            T = src_cut.size(1)                                     # 3. Get the current sequence length T\n","            mask = torch.tril(torch.ones(T, T, device=src.device)).unsqueeze(0)  # 4. Create a causal mask for the current sequence length T\n","            logits, _ = self.forward(src_cut, mask=mask)            # 5. Forward pass through the model to get logits\n","            logits = logits[:, -1, :] / temperature                 # 6. Focus on the last time step and apply temperature scaling\n","            if top_k is not None:                                   # 7. If top_k is specified, filter logits to keep only the top_k highest values\n","                v, _ = torch.topk(logits, top_k)                    #    - Get the top_k values and their indices\n","                logits[logits < v[:, [-1]]] = -float('Inf')         #    - Set logits below the top_k threshold to -Inf\n","            probs = F.softmax(logits, dim=-1)                       # 8. Apply softmax to obtain a probability distribution over the vocabulary\n","            next_token = torch.multinomial(probs, num_samples=1)    # 9. Sample the next token from this distribution using torch.multinomial\n","            src = torch.cat((src, next_token), dim=1)               # 10. Concatenate the sampled token to src for the next iteration\n","        return src\n"]},{"cell_type":"markdown","id":"88f8e53e","metadata":{"id":"88f8e53e"},"source":["Now that we have our model implemented, we can proceed to prepare the dataset and train it on some text data. We could use any publicly available text dataset already prepared for language modeling, but that would be too easy. Instead, we’ll create our own small dataset based on the poems of Adam Mickiewicz, one of the greatest Polish poets.\n","\n","Although the dataset will be relatively small, it should be sufficient for our educational purposes. It will consist of the following works:\n","\n","1. \"Pan Tadeusz\"\n","2. \"Dziady\" part II\n","3. \"Dziady\" part III\n","3. \"Dziady\" part IV\n","\n","All these poems are available in PDF format from [Wolne Lektury](https://wolnelektury.pl/katalog/autor/adam-mickiewicz/). To read the PDFs and extract the text, we will use the PyMuPDF library (you need to install it if you do). To remove unnecessary content such as prefaces, introductions, and appendices, we will skip the first and last pages of each PDF document.\n","\n","Since the texts are in Polish, we must also ensure that our tokenizer correctly handles Polish characters. In some PDFs, Polish letters may be read incorrectly due to encoding issues, so we may need to remap them from UTF-8 to Latin-1. Additionally, we will remove page numbers and any other extraneous elements.\n","\n","To generate the dataset, we will extract the text from the PDFs, clean it by removing unwanted characters and lines, and concatenate all the content into a single string. This text will then serve as the input for creating a custom dataset to train our Mini-GPT model.\n","\n","Of course, you could use different text data in any language, but I encourage you to try this approach with other works as well. Just make sure the text is in the public domain or that you have the right to use it for your experiments."]},{"cell_type":"code","execution_count":null,"id":"5e47a97d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":895,"status":"ok","timestamp":1760339692938,"user":{"displayName":"JAN ROSA","userId":"09972594920799623787"},"user_tz":-120},"id":"5e47a97d","outputId":"5f78aea4-a37f-440d-e150-9a51314c1729"},"outputs":[{"output_type":"stream","name":"stdout","text":["KSIĘGA PIERWSZA\n","Gospodarstwo\n","Powrót panicza — Spotkanie się pierwsze w pokoiku, drugie u stołu — Ważna\n","Sędziego nauka o grzeczności — Podkomorzego uwagi polityczne nad modami\n","— Początek sporu o Kusego i Sokoła — Żale Wojskiego — Ostatni Woźny\n","Trybunału — Rzut oka na ówczesny stan polityczny Litwy i Europy\n","Litwo! Ojczyzno moja! ty jesteś jak zdrowie:\n","Ile cię trzeba cenić, ten tylko się dowie,\n","Kto cię stracił. Dziś piękność twą w całej ozdobie\n","Widzę i opisuję, bo tęsknię po tobie.\n","Panno święta, co Jasnej bronisz Częstochowy\n","I w Ostrej świecisz Bramie! Ty, co gród zamkowy\n","Nowogródzki ochraniasz z jego wiernym ludem!\n","Jak mnie dziecko do zdrowia powróciłaś cudem\n","(Gdy od płaczącej matki, pod Twoją opiekę\n","Ofiarowany, martwą podniosłem powiekę;\n","I zaraz mogłem pieszo, do Twych świątyń progu\n","Iść za wrócone życie podziękować Bogu),\n","Tak nas powrócisz cudem na Ojczyzny łono.\n","Tymczasem przenoś moją duszę utęsknioną\n","Do tych pagórków leśnych, do tych łąk zielonych,\n","Szeroko nad błękitnym Niemnem rozcią\n","--------------------\n","Size of the text: 651513\n"]}],"source":["import fitz\n","\n","def read_pdf_fixed(path):\n","    doc = fitz.open(path)\n","    text = \"\"\n","    for page in doc[1:-1]:                  # skip first and last page\n","        page_text = page.get_text(\"text\")\n","        if page_text:\n","            text += page_text + \"\\n\"\n","\n","    # mapping of bad characters to good ones\n","    mapping = {\n","        \"Ê\": \"Ę\", \"ê\": \"ę\",\n","        \"Œ\": \"Ś\", \"œ\": \"ś\",\n","        \"³\": \"ł\", \"£\": \"Ł\",\n","        \"¯\": \"Ż\", \"¿\": \"ż\",\n","        \"Ÿ\": \"Ź\", \"ÿ\": \"ź\",\n","        \"ñ\": \"ń\", \"Ñ\": \"Ń\",\n","        \"¹\": \"ą\", \"¡\": \"Ą\",\n","        \"æ\": \"ć\", \"Æ\": \"Ć\",\n","        \"¢\": \"ó\",\n","    }\n","\n","    # replace bad characters\n","    for bad, good in mapping.items():\n","        text = text.replace(bad, good)\n","\n","    lines = text.splitlines()\n","    cleaned = []\n","    for line in lines:\n","        if line.strip().isdigit():          # remove page numbers\n","            continue\n","        cleaned.append(line)                # append cleaned line\n","\n","    return \"\\n\".join(cleaned)               # return cleaned text\n","\n","\n","# --- Read and clean text from multiple PDF files ---\n","text = \"\"\n","for file in [\"data/tadeusz.pdf\", \"data/dziady2.pdf\", \"data/dziady3.pdf\", \"data/dziady4.pdf\"]:\n","    text += read_pdf_fixed(file) + \"\\n\"\n","\n","\n","# --- Print a snippet of the cleaned text and its length ---\n","\n","print(text[:1000])\n","print(\"-\" * 20)\n","print(\"Size of the text:\", len(text))"]},{"cell_type":"markdown","id":"aa02b1fb","metadata":{"id":"aa02b1fb"},"source":["### Hyperparameters and Data Preparation\n","\n","In this part, we define the main hyperparameters and prepare the dataset for training our Mini-GPT model.\n","\n","First, we set parameters such as the batch size, embedding dimension, number of attention heads and layers, and the `block_size`, which defines how many previous tokens the model can see when predicting the next one.\n","\n","Next, we create functions to **encode** text into integers and **decode** integers back into text.  \n","We also build our vocabulary by finding all unique characters in the dataset.\n","\n","Then, we split the data into **training** and **validation** sets (90% / 10%).\n","\n","Finally, we define the `get_batch()` function, which randomly selects short sequences of tokens from the dataset.  \n","For each batch:\n","- `src` is the input sequence,  \n","- `tgt` is the same sequence shifted one step ahead, so the model learns to predict the next token.\n"]},{"cell_type":"code","execution_count":null,"id":"82d1f8f4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64,"status":"ok","timestamp":1760339693041,"user":{"displayName":"JAN ROSA","userId":"09972594920799623787"},"user_tz":-120},"id":"82d1f8f4","outputId":"351ba783-490a-4652-9479-88fc9a5b8d74"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1, 66, 52, 47, 69, 52, 62, 69,  1, 58, 61, 87, 68,  1, 69, 87, 58, 63,\n","        48,  6,  1, 62, 61, 48, 45, 61, 57, 48, 16,  1, 50, 47, 68,  1, 63, 61,\n","        85, 45, 44, 46, 69, 48,  0, 32, 58, 45, 64, 47, 54, 85,  1, 27, 58, 90,\n","        46, 52, 64, 62, 69, 54, 58, 66, 62, 54])\n","tensor([66, 52, 47, 69, 52, 62, 69,  1, 58, 61, 87, 68,  1, 69, 87, 58, 63, 48,\n","         6,  1, 62, 61, 48, 45, 61, 57, 48, 16,  1, 50, 47, 68,  1, 63, 61, 85,\n","        45, 44, 46, 69, 48,  0, 32, 58, 45, 64, 47, 54, 85,  1, 27, 58, 90, 46,\n","        52, 64, 62, 69, 54, 58, 66, 62, 54, 81])\n","--------------------\n"," widzisz orły złote, srebrne? gdy trębacze\n","Pobudkę Kościuszkowsk\n","widzisz orły złote, srebrne? gdy trębacze\n","Pobudkę Kościuszkowską\n"]}],"source":["# --- Hyperparameters ---\n","\n","batch_size = 16         # how many independent sequences will we process in parallel?\n","emb_size = 128          # embedding dimension for each token ()\n","num_heads = 8           # number of attention heads in each decoder block\n","num_layers = 6          # number of decoder blocks\n","block_size = 64         # maximum context length for predictions\n","\n","learning_rate = 1e-3    # learning rate\n","max_iters = 5000        # number of training iterations\n","eval_interval = 50      # interval for evaluation\n","eval_iters = 200        # number of iterations for evaluation\n","\n","# --- Prepare the encode/decode functions ---\n","\n","uniq_chars = sorted(list(set(text)))            # get all unique characters that occur in this text\n","vocab_size = len(uniq_chars)                    # the size of the vocabulary\n","\n","char2idx = { ch:i for i,ch in enumerate(uniq_chars) }       # mapping from characters to integers\n","idx2char = { i:ch for i,ch in enumerate(uniq_chars) }       # mapping from integers to characters\n","\n","encode = lambda seq: [char2idx[c] for c in seq]             # encoder: take a string, output a list of integers\n","decode = lambda seq: ''.join([idx2char[i] for i in seq])    # decoder: take a list of integers, output a string\n","\n","# --- Prepare the dataset ---\n","\n","data = torch.tensor(encode(text), dtype=torch.long)         # encode the entire text dataset and store it in a torch.Tensor\n","split = int(0.9*len(data))\n","train_data, val_data = data[:split], data[split:]           # split the data into train and validation sets\n","\n","# --- Function to get a batch of data ---\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data             # choose the dataset\n","    ix = torch.randint(0, len(data) - block_size, (batch_size,))    # random starting indices for the batch (from 0 to len(data) - block_size)\n","    src = data[ix.unsqueeze(1) + torch.arange(block_size)]          # get the sequences of length block_size for each index in the batch\n","    tgt = data[ix.unsqueeze(1) + torch.arange(1, block_size + 1)]   # targets are the same as src but shifted by one position\n","    return src, tgt\n","\n","src, tgt = get_batch('train')\n","\n","print(src[0])\n","print(tgt[0])\n","\n","print(\"-\" * 20)\n","\n","print(decode(src[0].tolist()))\n","print(decode(tgt[0].tolist()))"]},{"cell_type":"markdown","id":"017a6b15","metadata":{"id":"017a6b15"},"source":["### Model Initialization and Training Setup\n","\n","Here we create an instance of our **Mini-GPT** model and prepare the optimizer and learning rate scheduler.\n","\n","- The model is initialized with the vocabulary size and hyperparameters defined earlier, then moved to the selected device (CPU or GPU).\n","- We use the **AdamW** optimizer, which is well-suited for training transformer-based models due to its weight decay regularization.\n","- A **Cosine Annealing** learning rate scheduler gradually decreases the learning rate over time (`T_max = max_iters`), helping the model converge more smoothly during training.\n"]},{"cell_type":"code","execution_count":null,"id":"a1d0b79b","metadata":{"id":"a1d0b79b"},"outputs":[],"source":["model = MiniGPT(vocab_size, emb_size, num_heads, num_layers, block_size).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters)"]},{"cell_type":"code","execution_count":null,"id":"b7487428","metadata":{"id":"b7487428"},"outputs":[],"source":["class Trainer():\n","    def __init__(self,\n","                 model,\n","                 optimizer,\n","                 scheduler,\n","                 get_batch,\n","                 make_causal_mask,\n","                 device,\n","                 max_iters=5000,\n","                 eval_interval=50,\n","                 eval_batches=10,          # number of batches to average during evaluation\n","                 name=\"Jan Rosa\"):\n","\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","        self.get_batch = get_batch\n","        self.make_causal_mask = make_causal_mask\n","        self.device = device\n","        self.max_iters = max_iters\n","        self.eval_interval = eval_interval\n","        self.eval_batches = eval_batches\n","        self.name = name\n","        self.best_val_loss = float('inf')\n","\n","    def training_step(self):\n","        \"\"\"Perform one training step on a randomly sampled batch.\"\"\"\n","        self.model.train()\n","        src, target = self.get_batch('train')\n","        mask = self.make_causal_mask(src.size(1))\n","\n","        src, target, mask = src.to(self.device), target.to(self.device), mask.to(self.device)\n","\n","        logits, loss = self.model(src, target, mask)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        self.scheduler.step()\n","\n","        return loss.item()\n","\n","    def evaluate(self):\n","        \"\"\"Evaluate model performance on multiple validation batches.\"\"\"\n","        self.model.eval()\n","        total_loss = 0.0\n","\n","        with torch.no_grad():\n","            for _ in range(self.eval_batches):\n","                src, target = self.get_batch('val')\n","                mask = self.make_causal_mask(src.size(1))\n","                src, target, mask = src.to(self.device), target.to(self.device), mask.to(self.device)\n","\n","                _, val_loss = self.model(src, target, mask)\n","                total_loss += val_loss.item()\n","\n","        avg_loss = total_loss / self.eval_batches\n","        return avg_loss\n","\n","    def train(self):\n","        \"\"\"Main training loop with Weights & Biases logging and model checkpointing.\"\"\"\n","        wandb.init(\n","            project=\"lab5-mini-gpt\",\n","            entity=\"deep-neural-network-course\",\n","            group=\"mini-gpt\",\n","            name=self.name,\n","            settings=wandb.Settings(save_code=False)\n","        )\n","        wandb.watch(self.model, log=\"all\")\n","\n","        for step in range(self.max_iters):\n","            train_loss = self.training_step()\n","            wandb.log({\"train/loss\": train_loss, \"lr\": self.scheduler.get_last_lr()[0]}, step=step)\n","\n","            if step % self.eval_interval == 0:\n","                val_loss = self.evaluate()\n","                wandb.log({\"val/loss\": val_loss}, step=step)\n","                print(f\"Step [{step}/{self.max_iters}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","\n","                # Save best model\n","                if val_loss < self.best_val_loss:\n","                    self.best_val_loss = val_loss\n","                    torch.save(self.model.state_dict(), \"best_mini_gpt.pth\")\n","                    print(f\"New best model saved (Val Loss: {self.best_val_loss:.4f})\")\n","\n","        wandb.unwatch(model)\n","        wandb.finish()\n","\n","trainer = Trainer(model, optimizer, scheduler, get_batch, make_causal_mask, device, max_iters, eval_interval)"]},{"cell_type":"code","execution_count":null,"id":"79ffb957","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"79ffb957","executionInfo":{"status":"ok","timestamp":1760340068899,"user_tz":-120,"elapsed":374534,"user":{"displayName":"JAN ROSA","userId":"09972594920799623787"}},"outputId":"0aa36e87-508c-42b5-c0d8-86e962efc022"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.22.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251013_071454-91huu821</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/deep-neural-network-course/lab5-mini-gpt/runs/91huu821' target=\"_blank\">Jan Rosa</a></strong> to <a href='https://wandb.ai/deep-neural-network-course/lab5-mini-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/deep-neural-network-course/lab5-mini-gpt' target=\"_blank\">https://wandb.ai/deep-neural-network-course/lab5-mini-gpt</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/deep-neural-network-course/lab5-mini-gpt/runs/91huu821' target=\"_blank\">https://wandb.ai/deep-neural-network-course/lab5-mini-gpt/runs/91huu821</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Step [0/5000], Train Loss: 4.6548, Val Loss: 4.0054\n","New best model saved (Val Loss: 4.0054)\n","Step [50/5000], Train Loss: 2.6300, Val Loss: 2.6414\n","New best model saved (Val Loss: 2.6414)\n","Step [100/5000], Train Loss: 2.5962, Val Loss: 2.5665\n","New best model saved (Val Loss: 2.5665)\n","Step [150/5000], Train Loss: 2.3959, Val Loss: 2.4543\n","New best model saved (Val Loss: 2.4543)\n","Step [200/5000], Train Loss: 2.1470, Val Loss: 2.1710\n","New best model saved (Val Loss: 2.1710)\n","Step [250/5000], Train Loss: 1.2775, Val Loss: 1.2441\n","New best model saved (Val Loss: 1.2441)\n","Step [300/5000], Train Loss: 0.2776, Val Loss: 0.3264\n","New best model saved (Val Loss: 0.3264)\n","Step [350/5000], Train Loss: 0.1597, Val Loss: 0.1541\n","New best model saved (Val Loss: 0.1541)\n","Step [400/5000], Train Loss: 0.1076, Val Loss: 0.1145\n","New best model saved (Val Loss: 0.1145)\n","Step [450/5000], Train Loss: 0.1135, Val Loss: 0.0922\n","New best model saved (Val Loss: 0.0922)\n","Step [500/5000], Train Loss: 0.0628, Val Loss: 0.0952\n","Step [550/5000], Train Loss: 0.0800, Val Loss: 0.0823\n","New best model saved (Val Loss: 0.0823)\n","Step [600/5000], Train Loss: 0.0740, Val Loss: 0.0680\n","New best model saved (Val Loss: 0.0680)\n","Step [650/5000], Train Loss: 0.0589, Val Loss: 0.0728\n","Step [700/5000], Train Loss: 0.0439, Val Loss: 0.0621\n","New best model saved (Val Loss: 0.0621)\n","Step [750/5000], Train Loss: 0.0498, Val Loss: 0.0689\n","Step [800/5000], Train Loss: 0.0485, Val Loss: 0.0577\n","New best model saved (Val Loss: 0.0577)\n","Step [850/5000], Train Loss: 0.0512, Val Loss: 0.0558\n","New best model saved (Val Loss: 0.0558)\n","Step [900/5000], Train Loss: 0.0434, Val Loss: 0.0563\n","Step [950/5000], Train Loss: 0.0479, Val Loss: 0.0653\n","Step [1000/5000], Train Loss: 0.0488, Val Loss: 0.0599\n","Step [1050/5000], Train Loss: 0.0767, Val Loss: 0.0591\n","Step [1100/5000], Train Loss: 0.0379, Val Loss: 0.0540\n","New best model saved (Val Loss: 0.0540)\n","Step [1150/5000], Train Loss: 0.0563, Val Loss: 0.0610\n","Step [1200/5000], Train Loss: 0.0437, Val Loss: 0.0563\n","Step [1250/5000], Train Loss: 0.0420, Val Loss: 0.0576\n","Step [1300/5000], Train Loss: 0.0495, Val Loss: 0.0569\n","Step [1350/5000], Train Loss: 0.0392, Val Loss: 0.0509\n","New best model saved (Val Loss: 0.0509)\n","Step [1400/5000], Train Loss: 0.0535, Val Loss: 0.0538\n","Step [1450/5000], Train Loss: 0.0452, Val Loss: 0.0508\n","New best model saved (Val Loss: 0.0508)\n","Step [1500/5000], Train Loss: 0.0713, Val Loss: 0.0644\n","Step [1550/5000], Train Loss: 0.0526, Val Loss: 0.0599\n","Step [1600/5000], Train Loss: 0.0449, Val Loss: 0.0522\n","Step [1650/5000], Train Loss: 0.0467, Val Loss: 0.0581\n","Step [1700/5000], Train Loss: 0.0350, Val Loss: 0.0482\n","New best model saved (Val Loss: 0.0482)\n","Step [1750/5000], Train Loss: 0.0484, Val Loss: 0.0682\n","Step [1800/5000], Train Loss: 0.0423, Val Loss: 0.0444\n","New best model saved (Val Loss: 0.0444)\n","Step [1850/5000], Train Loss: 0.0397, Val Loss: 0.0515\n","Step [1900/5000], Train Loss: 0.0469, Val Loss: 0.0510\n","Step [1950/5000], Train Loss: 0.0412, Val Loss: 0.0504\n","Step [2000/5000], Train Loss: 0.0450, Val Loss: 0.0495\n","Step [2050/5000], Train Loss: 0.0334, Val Loss: 0.0542\n","Step [2100/5000], Train Loss: 0.0428, Val Loss: 0.0438\n","New best model saved (Val Loss: 0.0438)\n","Step [2150/5000], Train Loss: 0.0435, Val Loss: 0.0467\n","Step [2200/5000], Train Loss: 0.0442, Val Loss: 0.0456\n","Step [2250/5000], Train Loss: 0.0465, Val Loss: 0.0464\n","Step [2300/5000], Train Loss: 0.0495, Val Loss: 0.0475\n","Step [2350/5000], Train Loss: 0.0465, Val Loss: 0.0555\n","Step [2400/5000], Train Loss: 0.0372, Val Loss: 0.0413\n","New best model saved (Val Loss: 0.0413)\n","Step [2450/5000], Train Loss: 0.0366, Val Loss: 0.0470\n","Step [2500/5000], Train Loss: 0.0390, Val Loss: 0.0394\n","New best model saved (Val Loss: 0.0394)\n","Step [2550/5000], Train Loss: 0.0386, Val Loss: 0.0464\n","Step [2600/5000], Train Loss: 0.0360, Val Loss: 0.0498\n","Step [2650/5000], Train Loss: 0.0346, Val Loss: 0.0462\n","Step [2700/5000], Train Loss: 0.0429, Val Loss: 0.0482\n","Step [2750/5000], Train Loss: 0.0415, Val Loss: 0.0434\n","Step [2800/5000], Train Loss: 0.0504, Val Loss: 0.0415\n","Step [2850/5000], Train Loss: 0.0379, Val Loss: 0.0455\n","Step [2900/5000], Train Loss: 0.0317, Val Loss: 0.0465\n","Step [2950/5000], Train Loss: 0.0470, Val Loss: 0.0406\n","Step [3000/5000], Train Loss: 0.0432, Val Loss: 0.0411\n","Step [3050/5000], Train Loss: 0.0301, Val Loss: 0.0420\n","Step [3100/5000], Train Loss: 0.0363, Val Loss: 0.0448\n","Step [3150/5000], Train Loss: 0.0481, Val Loss: 0.0434\n","Step [3200/5000], Train Loss: 0.0418, Val Loss: 0.0430\n","Step [3250/5000], Train Loss: 0.0365, Val Loss: 0.0451\n","Step [3300/5000], Train Loss: 0.0357, Val Loss: 0.0443\n","Step [3350/5000], Train Loss: 0.0469, Val Loss: 0.0433\n","Step [3400/5000], Train Loss: 0.0330, Val Loss: 0.0414\n","Step [3450/5000], Train Loss: 0.0406, Val Loss: 0.0446\n","Step [3500/5000], Train Loss: 0.0303, Val Loss: 0.0418\n","Step [3550/5000], Train Loss: 0.0261, Val Loss: 0.0422\n","Step [3600/5000], Train Loss: 0.0352, Val Loss: 0.0433\n","Step [3650/5000], Train Loss: 0.0398, Val Loss: 0.0392\n","New best model saved (Val Loss: 0.0392)\n","Step [3700/5000], Train Loss: 0.0427, Val Loss: 0.0392\n","Step [3750/5000], Train Loss: 0.0345, Val Loss: 0.0448\n","Step [3800/5000], Train Loss: 0.0310, Val Loss: 0.0402\n","Step [3850/5000], Train Loss: 0.0334, Val Loss: 0.0435\n","Step [3900/5000], Train Loss: 0.0306, Val Loss: 0.0449\n","Step [3950/5000], Train Loss: 0.0410, Val Loss: 0.0437\n","Step [4000/5000], Train Loss: 0.0306, Val Loss: 0.0404\n","Step [4050/5000], Train Loss: 0.0348, Val Loss: 0.0437\n","Step [4100/5000], Train Loss: 0.0245, Val Loss: 0.0440\n","Step [4150/5000], Train Loss: 0.0313, Val Loss: 0.0477\n","Step [4200/5000], Train Loss: 0.0308, Val Loss: 0.0433\n","Step [4250/5000], Train Loss: 0.0407, Val Loss: 0.0377\n","New best model saved (Val Loss: 0.0377)\n","Step [4300/5000], Train Loss: 0.0454, Val Loss: 0.0404\n","Step [4350/5000], Train Loss: 0.0329, Val Loss: 0.0375\n","New best model saved (Val Loss: 0.0375)\n","Step [4400/5000], Train Loss: 0.0275, Val Loss: 0.0415\n","Step [4450/5000], Train Loss: 0.0389, Val Loss: 0.0390\n","Step [4500/5000], Train Loss: 0.0393, Val Loss: 0.0352\n","New best model saved (Val Loss: 0.0352)\n","Step [4550/5000], Train Loss: 0.0344, Val Loss: 0.0365\n","Step [4600/5000], Train Loss: 0.0314, Val Loss: 0.0366\n","Step [4650/5000], Train Loss: 0.0285, Val Loss: 0.0443\n","Step [4700/5000], Train Loss: 0.0280, Val Loss: 0.0411\n","Step [4750/5000], Train Loss: 0.0396, Val Loss: 0.0391\n","Step [4800/5000], Train Loss: 0.0293, Val Loss: 0.0409\n","Step [4850/5000], Train Loss: 0.0426, Val Loss: 0.0429\n","Step [4900/5000], Train Loss: 0.0359, Val Loss: 0.0370\n","Step [4950/5000], Train Loss: 0.0352, Val Loss: 0.0416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>█████████▇▇▇▇▇▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>███▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/loss</td><td>██▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0</td></tr><tr><td>train/loss</td><td>0.03863</td></tr><tr><td>val/loss</td><td>0.0416</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">Jan Rosa</strong> at: <a href='https://wandb.ai/deep-neural-network-course/lab5-mini-gpt/runs/91huu821' target=\"_blank\">https://wandb.ai/deep-neural-network-course/lab5-mini-gpt/runs/91huu821</a><br> View project at: <a href='https://wandb.ai/deep-neural-network-course/lab5-mini-gpt' target=\"_blank\">https://wandb.ai/deep-neural-network-course/lab5-mini-gpt</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251013_071454-91huu821/logs</code>"]},"metadata":{}}],"source":["trainer = Trainer(\n","    model=model,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    get_batch=get_batch,\n","    make_causal_mask=make_causal_mask,\n","    device=device,\n","    max_iters=max_iters,\n","    eval_interval=eval_interval,\n","    name=\"Jan Rosa\"\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"724ca0e6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"724ca0e6","executionInfo":{"status":"ok","timestamp":1760340090862,"user_tz":-120,"elapsed":21960,"user":{"displayName":"JAN ROSA","userId":"09972594920799623787"}},"outputId":"9737b14f-4412-4b7b-938d-515b5766d853"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Quuuuxx ’’’’’’’’jjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjnaj się się się wych się wychanie.\n","W szara nie się się się powarze w się zajech poszerz wieszczy szlaczy powierzy,\n","I pował się zarasze na się nie się szarzenie\n","I przedzie się szeszczał się się na przecznał\n","Stata starz zaja z w zasta zarze się na przecz zaja.\n","Stak szarał szarzecz wieszczy w się się szerzyła,\n","A Moski się na się się na się powarał się przedzie,\n","A Mickiem się przecz szaranie się szerzenie,\n","I z się szlecz szarał wieszy się się powiecze,\n","A chała się na przesz, czy się się się się powała,\n","A chara się szaranie na się na się na się powanie\n","I zaradzie zasta szlani się zarasze zajak się na postan podzieczał,\n","A Moskie się nie się nie się przedział,\n","Pan przed się szarze szarze się na szarał,\n","A charze się się się nie się szeraz szlanie\n","I powie wiesz powarze się szerzy powiedział.\n","Przeszczy się zajesz szarzecz się się na się szawie.\n","Sada przed się nie się szaradzi na się nia się powała.\n","A Mickiem się się się nie szała się \n"]}],"source":["# generate from the model\n","model.load_state_dict(torch.load(\"best_mini_gpt.pth\"))\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","generated = model.generate(context, max_new_tokens=1000, temperature=0.5, top_k=2)[0].tolist()\n","print(decode(generated))"]},{"cell_type":"code","execution_count":null,"id":"8bb3a74d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8bb3a74d","executionInfo":{"status":"ok","timestamp":1760340101833,"user_tz":-120,"elapsed":10969,"user":{"displayName":"JAN ROSA","userId":"09972594920799623787"}},"outputId":"ca385e9c-6768-4e70-b1e4-631160ea5eca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Litwo! Ojczyzno moja!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","I czy nie jąc pote! niecz z mięże je na jacha!\n","I za w się się szyszał przeka do piatan,\n","A jaszny mędzicz się mierzem na sprzedział postanie się na wajchata stronie\n","I wszysz wa na panki drony w czyszczy się potwają,\n","A Moszem malich się porzech podziej w sponiecia,\n","Ale ja drowie wstarzewo z ponoch wychwało czychem stałam,\n","I w dzie kowszy z sza nał powszy sianie,\n","Czała, czy weszy nie miają się panem się nie na kamiądam dam niecie,\n","A chcy na porzeszy wszys\n"]}],"source":["input_text = \"Litwo! Ojczyzno moja!\"\n","context = torch.tensor(encode(input_text), dtype=torch.long, device=device).unsqueeze(0)\n","generated = model.generate(context, max_new_tokens=500, temperature=0.7, top_k=10)[0].tolist()\n","print(decode(generated))"]},{"cell_type":"code","execution_count":null,"id":"33bbdba1","metadata":{"id":"33bbdba1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760340101840,"user_tz":-120,"elapsed":5,"user":{"displayName":"JAN ROSA","userId":"09972594920799623787"}},"outputId":"ac1c62e1-f3e6-400a-abc6-aeba5bd01269"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model architecture:\n","MiniGPT(\n","  (token_emb): Embedding(100, 128)\n","  (pos_emb): PositionalEncoding()\n","  (blocks): ModuleList(\n","    (0-5): 6 x DecoderBlock(\n","      (self_attn): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-7): 8 x Head(\n","            (key): Linear(in_features=128, out_features=16, bias=False)\n","            (query): Linear(in_features=128, out_features=16, bias=False)\n","            (value): Linear(in_features=128, out_features=16, bias=False)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (linear): Linear(in_features=128, out_features=128, bias=False)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (ff): Sequential(\n","        (0): Linear(in_features=128, out_features=512, bias=True)\n","        (1): ReLU()\n","        (2): Linear(in_features=512, out_features=128, bias=True)\n","        (3): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","  )\n","  (fc_out): Linear(in_features=128, out_features=100, bias=True)\n",")\n","Number of parameters: 1.21226 M\n"]}],"source":["print(\"Model architecture:\")\n","print(model)\n","\n","print(\"Number of parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad) / 1_000_000, \"M\")"]},{"cell_type":"markdown","id":"7f4caa14","metadata":{"id":"7f4caa14"},"source":["As we can see, we now have our own Mini-GPT model capable of generating fairly coherent text in Polish, despite being trained on a small dataset. The model itself is quite compact — with only about **1.2 million parameters**, it can be easily trained on a single GPU.\n","\n","For comparison, the original GPT-3 model contains around **175 billion parameters**, while GPT-4 is estimated to have about **1.8 trillion parameters** (although during inference it uses only a subset of them thanks to the Mixture of Experts architecture). This highlights just how much smaller and lighter our implementation is.\n","\n","One potential improvement would be to cache the results of the attention mechanism during text generation. This would allow the model to avoid recomputing attention outputs for all previously generated tokens at each step — a significant optimization for longer sequences. However, for the sake of simplicity, we did not implement this feature here."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.23"}},"nbformat":4,"nbformat_minor":5}